BATCH_SIZE = 64
# Drop 10%
DROPOUT_PROB = 0.1
# The width of the embedding vectors
EMBED_DIM = 48
GRAD_NORM_CLIP = 1.0
# The number of attention heads in each multi-head attention block
NUM_ATTN_HEADS = 3
# The number of transformer blocks
NUM_BLOCKS = 3
NUM_SAMPLES = int(1e10)
WEIGHT_DECAY = 0.1
