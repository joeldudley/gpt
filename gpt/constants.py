BATCH_SIZE = 64
# Drop 10%
DROPOUT_PROB = 0.1
# The width of the embedding vectors
EMBED_DIM = 48
MAX_GRAD_NORM = 1.0
# The number of attention heads in each multi-head attention block
NUM_ATTN_HEADS = 3
# The number of transformer blocks
NUM_BLOCKS = 3
NUM_SAMPLES = int(1e10)
# Standard deviation to use for normal distributions
STD = 0.02
WEIGHT_DECAY = 0.1
