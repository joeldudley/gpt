BATCH_SIZE = 64
BETAS = (0.9, 0.95)
# Drop 10%
DROPOUT_PROB = 0.1
# The width of the embedding vectors
EMBED_DIM = 48
GRAD_NORM_CLIP = 1.0
LEARNING_RATE = 5e-4
# The number of attention heads in each multi-head attention block
NUM_ATTN_HEADS = 3
# The number of transformer blocks
NUM_BLOCKS = 3
NUM_WORKERS = 4
SAMPLES = int(1e10)
TEMPERATURE = 1.0
WEIGHT_DECAY = 0.1
